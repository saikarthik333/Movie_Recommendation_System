# -*- coding: utf-8 -*-
"""movie_recommendation (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11oB9-6-2B1I4YcP5A5slcRbCCjoWCrea
"""

import pandas as pd
import numpy as np

from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors

import matplotlib.pyplot as plt
import seaborn as sns

movies = pd.read_csv("/content/movies.csv", on_bad_lines='warn')
ratings = pd.read_csv("/content/ratings.csv")

# movies = pd.read_csv("/content/movies.csv")
# ratings = pd.read_csv("/content/ratings.csv")

ratings.head()

movies.head()

"""Movie dataset has
* movieId - once the recommendation is done, we get list of all similar movieId and get the title for each movie from this dataset.
* genres -  which is not required for this filtering approach
"""

final_dataset = ratings.pivot_table(index='movieId',columns='userId',values='rating')
final_dataset.head()

final_dataset.fillna(0,inplace=True)
final_dataset.head()

"""In a real world, ratings are very sparse and data points are mostly collected from very popular movies and highly engaged users. So we will reduce the noise by adding some filters and qualify the movies for the final dataset.
* To qualify a movie, minimum 10 users should have voted a movie.
* To qualify a user, minimum 50 movies should have voted by the user.

"""

no_user_voted = ratings.groupby('movieId')['rating'].agg('count')
no_movies_voted = ratings.groupby('userId')['rating'].agg('count')

no_user_voted = ratings.groupby('movieId')['rating'].agg('count')
no_movies_voted = ratings.groupby('userId')['rating'].agg('count')

f,ax = plt.subplots(1,1,figsize=(16,4))
# ratings['rating'].plot(kind='hist')
plt.scatter(no_user_voted.index,no_user_voted,color='mediumseagreen')
plt.axhline(y=10,color='r')
plt.xlabel('MovieId')
plt.ylabel('No. of users voted')
plt.show()

final_dataset = final_dataset.loc[no_user_voted[no_user_voted > 10].index,:]

f,ax = plt.subplots(1,1,figsize=(16,4))
plt.scatter(no_movies_voted.index,no_movies_voted,color='mediumseagreen')
plt.axhline(y=50,color='r')
plt.xlabel('UserId')
plt.ylabel('No. of votes by user')
plt.show()

# # prompt: describe the above graph

# The code performs exploratory data analysis and pre-processing on movie rating data to prepare it for a recommendation system, likely using collaborative filtering.  Let's break down the key steps:

# 1. **Data Loading and Initial Inspection:**
#    - It loads two CSV files: `movies.csv` (containing movie IDs and genres) and `ratings.csv` (containing user ratings for movies).
#    - It displays the first few rows of the `ratings` DataFrame to show the structure of the data.

# 2. **Data Pivoting:**
#    - `final_dataset = ratings.pivot(index='movieId',columns='userId',values='rating')` creates a pivot table where rows represent movies, columns represent users, and the cell values are the ratings.  This transforms the data into a user-item interaction matrix.

# 3. **Handling Missing Values:**
#    - `final_dataset.fillna(0,inplace=True)` replaces missing ratings (where a user hasn't rated a movie) with 0.  This is a common approach, but other imputation methods might be considered.

# 4. **Data Filtering (Noise Reduction):**
#    - The code aims to reduce noise in the data by applying two filters:
#      - **Movies:**  Only movies with at least 10 ratings are kept.  This removes movies with very few ratings, which are less reliable for recommendations.
#      - **Users:** Only users who have rated at least 50 movies are kept.  This removes less active users whose ratings might be less representative.

# 5. **Visualization of Filtering Criteria:**
#    - The code generates two scatter plots:
#      - The first shows the number of users who rated each movie.  A horizontal red line at `y=10` visualizes the threshold for including a movie.
#      - The second shows the number of movies rated by each user. A horizontal red line at `y=50` visualizes the threshold for including a user.

#    - These plots help to visualize the distribution of ratings and the impact of the chosen filters.

# **In essence, the code prepares the movie rating data for a collaborative filtering recommendation system by:**

# * Transforming the data into a user-item matrix.
# * Handling missing values.
# * Filtering out movies and users with very few ratings to improve the quality of the recommendations.


# The code sets up the data, but the actual recommendation algorithm (e.g., k-Nearest Neighbors, matrix factorization) is not implemented here.  The next logical steps would involve applying a collaborative filtering algorithm to the `final_dataset`.

final_dataset=final_dataset.loc[:,no_movies_voted[no_movies_voted > 50].index]
final_dataset

sample = np.array([[0,0,3,0,0],[4,0,0,0,2],[0,0,0,0,1]])
sparsity = 1.0 - ( np.count_nonzero(sample) / float(sample.size) )
print(sparsity)

csr_sample = csr_matrix(sample)
print(csr_sample)

csr_data = csr_matrix(final_dataset.values)
final_dataset.reset_index(inplace=True)

"""We use cosine distance metric which is very fast and preferable than pearson coefficient."""

knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)

knn = NearestNeighbors(metric='cosine', algorithm='brute', n_neighbors=20, n_jobs=-1)
knn.fit(csr_data)

def get_movie_recommendation(movie_name):
    n_movies_to_reccomend = 10
    movie_list = movies[movies['title'].str.contains(movie_name)]
    if len(movie_list):
        movie_idx= movie_list.iloc[0]['movieId']
        movie_idx = final_dataset[final_dataset['movieId'] == movie_idx].index[0]

        distances , indices = knn.kneighbors(csr_data[movie_idx],n_neighbors=n_movies_to_reccomend+1)
        rec_movie_indices = sorted(list(zip(indices.squeeze().tolist(),distances.squeeze().tolist())),\
                               key=lambda x: x[1])[:0:-1]

        recommend_frame = []

        for val in rec_movie_indices:
            movie_idx = final_dataset.iloc[val[0]]['movieId']
            idx = movies[movies['movieId'] == movie_idx].index
            recommend_frame.append({'Title':movies.iloc[idx]['title'].values[0],'Distance':val[1]})
        df = pd.DataFrame(recommend_frame,index=range(1,n_movies_to_reccomend+1))
        return df

    else:

        return "No movies found. Please check your input"

get_movie_recommendation('Iron Man')

get_movie_recommendation('Iron Man 2')

"""model works perfectly predicting the recommendation based on user behaviour and past search. So we conclude our
collaborative filtering here.

"""
